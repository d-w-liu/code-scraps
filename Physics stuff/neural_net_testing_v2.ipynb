{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import awkward\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "noise_events = 5000\n",
    "signal_events = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\program files (x86)\\python 3.7\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "noise_data_x = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 1.0, correlation = 0.0, width_factor = 1.0)\n",
    "noise_data_y = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 1.2, correlation = 0.0, width_factor = 1.0)\n",
    "noise_data_z = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 0.8, correlation = 0.0, width_factor = 1.0)\n",
    "\n",
    "signal_data_x = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "signal_data_y = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "signal_data_z = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "\n",
    "noise_data_x = noise_data_x[0].tolist()\n",
    "noise_data_y = noise_data_y[0].tolist()\n",
    "noise_data_z = noise_data_z[0].tolist()\n",
    "\n",
    "signal_data_x = signal_data_x[0].tolist()\n",
    "signal_data_y = signal_data_y[0].tolist()\n",
    "signal_data_z = signal_data_z[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_list = [0] * noise_events\n",
    "signal_list = [1] * signal_events\n",
    "\n",
    "var_x = noise_data_x + signal_data_x\n",
    "var_y = noise_data_y + signal_data_y\n",
    "var_z = noise_data_z + signal_data_z\n",
    "signal_or_noise_axis_1 = noise_list + signal_list\n",
    "signal_or_noise_axis_2 = noise_list + signal_list\n",
    "\n",
    "data_dict = {'var_1':var_x, 'var_2':var_y, 'var_3':var_z}\n",
    "output_dict = {'sn1':signal_or_noise_axis_1, 'sn2':signal_or_noise_axis_2}\n",
    "\n",
    "# Construct a pandas dataframe with all our data and prepare it for analysis:\n",
    "\n",
    "input_data = pd.DataFrame(data_dict)\n",
    "output_data = pd.DataFrame(output_dict)\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"scale\")\n",
    "neural_network = MultiOutputClassifier(estimator=svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=SVC())"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(neural_network.score(input_train, output_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neural_network.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       380\n",
      "           1       1.00      1.00      1.00       380\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       760\n",
      "   macro avg       1.00      1.00      1.00       760\n",
      "weighted avg       1.00      1.00      1.00       760\n",
      " samples avg       0.23      0.23      0.23       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\program files (x86)\\python 3.7\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\program files (x86)\\python 3.7\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(output_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\program files (x86)\\python 3.7\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "noise_data_x = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 1.0, correlation = 0.0, width_factor = 1.0)\n",
    "noise_data_y = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 1.2, correlation = 0.0, width_factor = 1.0)\n",
    "noise_data_z = generate_distribution(events = 5000, peak = 0.0, width = 10.0, height = 0.8, correlation = 0.0, width_factor = 1.0)\n",
    "\n",
    "signal_data_x = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "signal_data_y = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "signal_data_z = generate_distribution(events = 1500, peak = 20.0, width = 10.0, height = 3.0, correlation = 0.4, width_factor = 1.0)\n",
    "\n",
    "test_noise_data_x = noise_data_x[1].tolist()\n",
    "test_noise_data_y = noise_data_y[1].tolist()\n",
    "test_noise_data_z = noise_data_z[1].tolist()\n",
    "\n",
    "test_signal_data_x = signal_data_x[1].tolist()\n",
    "test_signal_data_y = signal_data_y[1].tolist()\n",
    "test_signal_data_z = signal_data_z[1].tolist()\n",
    "\n",
    "new_var_x = test_signal_data_x + test_noise_data_x\n",
    "new_var_y = test_signal_data_y + test_noise_data_y\n",
    "new_var_z = test_signal_data_z + test_noise_data_z\n",
    "\n",
    "signal_or_noise_axis_3 = signal_list + noise_list\n",
    "signal_or_noise_axis_4 = signal_list + noise_list\n",
    "\n",
    "test_data_dict = {'var_1':new_var_x, 'var_2':new_var_y, 'var_3':new_var_z}\n",
    "test_snr_dict = {'sn3':signal_or_noise_axis_3, 'sn4':signal_or_noise_axis_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_data = pd.DataFrame(test_data_dict)\n",
    "output_test_data = pd.DataFrame(test_snr_dict)\n",
    "\n",
    "new_predictions = neural_network.predict(input_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for predicted in predictions:\n",
    "    if predicted.any() == 1:\n",
    "        count = count + 1\n",
    "print(count)\n",
    "\n",
    "new_count = 0\n",
    "for predicted in new_predictions:\n",
    "    if predicted.any() == 1:\n",
    "        new_count = new_count+1\n",
    "print(new_count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_distribution(events, peak, width, height, correlation, width_factor):\n",
    "    x_interval = np.array([peak - (width/2), peak + (width/2)])\n",
    "    y_interval = np.array([0, height])\n",
    "    vector = np.vstack((x_interval, y_interval)).T\n",
    "\n",
    "    means = lambda x, y : [x.mean(), y.mean()]\n",
    "    stdvs = lambda x, y : [x.std() / width_factor, y.std() / width_factor]\n",
    "    covs = [[stdvs(x_interval, y_interval)[0]**2, stdvs(x_interval, y_interval)[0] * stdvs(x_interval, y_interval)[1] * correlation],\n",
    "            [stdvs(x_interval, y_interval)[1] * stdvs(x_interval, y_interval)[1] * correlation, stdvs(x_interval, y_interval)[1]**2]]\n",
    "\n",
    "    generated_data = np.random.multivariate_normal(means(x_interval, y_interval), covs, events).T\n",
    "\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
